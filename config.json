{
    "vocab_size":1,
    "hidden_size": 32,
    "num_hidden_layers": 9,
    "num_attention_heads":8,
    "intermediate_size": 64,
    "act_fn":"gelu",
    "initializer_range": 0.02,
    "dropout": 0.1,
    "hidden_dropout_prob": 0.1,
    "attn_dropout": 0.1,
    "seq_len": 400
  }