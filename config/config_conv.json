{
    "vocab_size":1,
    "hidden_size": 64,
    "num_hidden_layers": 4,
    "num_attention_heads":8,
    "intermediate_size": 128,
    "act_fn":"leakyrelu",
    "initializer_range": 0.02,
    "dropout": 0.1,
    "hidden_dropout_prob": 0.1,
    "attn_dropout": 0.1,
    "seq_len": 1000,
    "hidden_len":512
  }